import re
import torch

JUDGE_PROMPT_TEMPLATE = """
ç”¨æˆ·è¾“å…¥: {{user_query}}

# ä»»åŠ¡
ä»…è¾“å‡ºYESæˆ–NOï¼Œåˆ¤å®šæ˜¯å¦ä¸ºç¾é£Ÿç›¸å…³è¯é¢˜ï¼ˆåŒ…æ‹¬ï¼šå¯é£Ÿç”¨çš„é£Ÿç‰©/é£Ÿæ/èœç³»/æ–™ç†/æ˜†è™«ï¼Œåˆ¶ä½œæ–¹æ³•ï¼Œé¥®é£Ÿéœ€æ±‚/è¥å…»æ¨èï¼Œé£Ÿææ­é…ï¼‰ã€‚

# å…³é”®ç¤ºä¾‹ï¼ˆå¿…é¡»å‚è€ƒï¼‰
âœ… YESï¼š
- how to make Sichuan cuisineï¼ˆå·èœæ˜¯é£Ÿç‰©ï¼‰
- Italian dishesï¼ˆæ„å¤§åˆ©èœæ˜¯é£Ÿç‰©ï¼‰
- How to cook insectï¼ˆæ˜†è™«å¯é£Ÿç”¨ï¼‰
- make ice creamï¼ˆåˆ¶ä½œé£Ÿç‰©ï¼‰
- bake pizzaï¼ˆçƒ¤æŠ«è¨ï¼‰
- 30 minute kwaiï¼ˆå¿«æ‰‹èœï¼‰
- high-protein low-carb foodsï¼ˆé«˜è›‹ç™½ä½ç¢³æ°´é£Ÿç‰©æ¨èï¼‰
- vegan dishes under 300 caloriesï¼ˆä½å¡ç´ é£Ÿæ¨èï¼‰
- food for post-workout recoveryï¼ˆå¥èº«åé£Ÿç‰©ï¼‰

âŒ NOï¼š
- ä¸å¯é£Ÿç”¨çš„äºº/ç‰©ï¼ˆå¦‚trumpã€phoneã€stoneã€rockï¼‰
- ä¸é£Ÿç‰©æ— å…³çš„å†…å®¹ï¼ˆå¦‚how are youï¼‰
- ä¸é£Ÿç‰©æ— å…³çš„é—²èŠ/æƒ…ç»ªå‘æ³„ï¼ˆæ— é£Ÿç‰©æ ¸å¿ƒè¯ï¼‰

åªè¾“å‡ºYESæˆ–NOï¼
"""

def is_food_query_final_solution(userquery, model, tokenizer):
    # æ­¥éª¤1ï¼šæ¸…ç†æƒ…ç»ªè¯/è„è¯ï¼Œä¿ç•™æ ¸å¿ƒè¯­ä¹‰
    dirty_words = {'fuck', 'ass', 'shit', 'bitch', 'hate', 'omfg'}
    userquery_clean = userquery.strip().lower()
    for word in dirty_words:
        userquery_clean = re.sub(rf'\b{word}\b', '', userquery_clean)
    userquery_clean = re.sub(r'\s+', ' ', userquery_clean).strip()
    userquery_clean = re.sub(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>\/?]', '', userquery_clean)

    # æ­¥éª¤2ï¼šåŠ¨ä½œ+é£Ÿç‰©å¯¹è±¡è§„åˆ™ï¼ˆè¡¥å……é¥®é£Ÿéœ€æ±‚ç›¸å…³åŠ¨ä½œï¼‰
    food_actions = {'make', 'cook', 'eat', 'prepare', 'bake', 'fry', 'crave', 'recommend', 'suggest', 'need'}  # æ–°å¢recommend/suggest/need
    food_objects = {
        'ice cream', 'cake', 'dish', 'food', 'meal', 'snack', 'insect', 'bug',
        'pizza', 'rice', 'vegetables', 'quinoa', 'protein', 'carb', 'calories', 'vegan'  # æ–°å¢è¥å…»æœ¯è¯­
    }
    words = userquery_clean.split()
    has_food_action = any(action in words for action in food_actions)
    has_food_object = any(obj in words for obj in food_objects)
    if has_food_action and has_food_object:
        print(f"ğŸ” åŠ¨ä½œ+é£Ÿç‰©å¯¹è±¡åŒ¹é…ï¼š[{userquery}] â†’ è¾“å‡ºï¼š[YES]")
        return 'YES'

    # æ­¥éª¤3ï¼šå¼±è§„åˆ™è¿‡æ»¤ï¼ˆè¡¥å……é¥®é£Ÿéœ€æ±‚/è¥å…»å…³é”®è¯ï¼‰
    clear_food_words = {
        'food', 'meal', 'dish', 'snack', 'ingredient', 'recipe',
        'cuisine', 'cuisines', 'dishes',
        'hungry', 'starving', 'craving',
        'cheesecake', 'chess cake', 'milkshake', 'milk shake',
        'insect', 'edible insect', 'bug', 'edible bug', 'quinoa',
        # æ–°å¢é¥®é£Ÿéœ€æ±‚/è¥å…»å…³é”®è¯
        'protein', 'carb', 'carbs', 'calorie', 'calories', 'low-carb', 'high-protein',
        'vegan', 'vegetarian', 'keto', 'diet', 'weight loss', 'post-workout', 'recovery',
        'kung pao', 'picnic'
    }
    clear_non_food_words = {
        'trump', 'biden', 'phone', 'car', 'money', 'computer', 'book', 'stone', 'rock', 'dog',
        'toy car', 'homework', 'doll', 'video game', 'pencil', 'crayon', 'weapon', 'weapons', 'gta5'
    }

    # å…³é”®è°ƒæ•´ï¼šå…ˆåˆ¤å®šéé£Ÿç‰©è¯ï¼Œå†åˆ¤å®šé£Ÿç‰©è¯
    if any(word in userquery_clean for word in clear_non_food_words):
        print(f"ğŸ” æ˜ç¡®éé£Ÿç‰©è¯åŒ¹é…ï¼š[{userquery}] â†’ è¾“å‡ºï¼š[NO]")
        return 'NO'
    if any(word in userquery_clean for word in clear_food_words):
        print(f"ğŸ” æ˜ç¡®é£Ÿç‰©è¯åŒ¹é…ï¼š[{userquery}] â†’ è¾“å‡ºï¼š[YES]")
        return 'YES'

    # æ­¥éª¤4ï¼šæ¨¡å‹åˆ¤å®šï¼ˆæç¤ºè¯å·²è¡¥å……é¥®é£Ÿéœ€æ±‚ç¤ºä¾‹ï¼‰
    judge_prompt = JUDGE_PROMPT_TEMPLATE.replace("{{user_query}}", userquery_clean)
    with torch.no_grad():
        inputs = tokenizer.apply_chat_template(
            [
                {'role':"system", "content": 'å¿½ç•¥è„è¯ã€æƒ…ç»ªè¯ã€æ— å…³äº‹ä»¶ï¼Œåªçœ‹æ ¸å¿ƒè¯­ä¹‰æ˜¯å¦ä¸ºé£Ÿç‰©/é¥®é£Ÿéœ€æ±‚/è¥å…»æ¨èï¼›åªè¾“å‡ºYES/NO'},
                {"role": "user", "content": judge_prompt},
            ],
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)

        outputs = model.generate(
            inputs,
            max_new_tokens=2,
            temperature=0.0,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    raw_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()
    clean_output = re.sub(r'[ã€‚.\s]', '', raw_output).upper()
    print(f"ğŸ” æ¨¡å‹æ ¸å¿ƒåˆ¤å®šï¼š[{userquery}] â†’ è¾“å‡ºï¼š[{clean_output}]")
    return clean_output